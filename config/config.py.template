import time

# Variables in ALL_CAPS are used by the testing framework, while variables in lower_case are
# used locally to set up other variables.


# Standard Configuration Options
# --------------------------------------------------

# Git commit id to checkout and test. The repository location configured via SPARK_GIT_REPO is 
# named "origin". Other than a hash code, you can also provide a branch name, or to test a pull 
# request, give a string of the format origin/pr/N to checkout pull request N.
COMMIT_ID = ""

# File to write results to.
OUTPUT_FILENAME = "spark_perf_output_%s" % time.strftime("%Y-%m-%d_%H-%M-%S") 

SPARK_GIT_REPO = "git://github.com/mesos/spark.git"

# The test framework clones its own clean copy of Spark, and then the files in this dir (e.g.
# spark-env.sh and slaves file) are copied from this directory to that Spark's conf dir before we
# start the Spark. To test on your local machine, create a spark-env.sh and a slaves file with a
# single slave set as your local machine.
SPARK_CONF_DIR = "/root/spark/conf"

# This default setting assumes we are running on the Spark EC2 AMI. Developers will probably want
# to change this to SPARK_CLUSTER_URL = "spark://localhost:7077" for testing.
SPARK_CLUSTER_URL = open("/root/spark-ec2/cluster-url", 'r').readline().strip()

# Command used to launch Scala.
SCALA_CMD = "scala"

# The default values configured below are appropriate for approximately 20 m1.xlarge nodes.
# Use this variable to scale the values if you are running the tests with more or fewer nodes.
scale_factor = 1.0


# Test Configuration
# ----------------------------------------------------------------------------------------

# Represents a Java option and a set of values for it that we will sweep over in a test run.
class JavaOptionSet:
  def __init__(self, name, vals):
    self.name = name
    self.vals = vals
  def toArray(self):
    return ["-D%s=%s" % (self.name, val) for val in self.vals]

# Represents an option and a set of values for that option that we will sweep over in a test run.
class OptionSet:
  def __init__(self, name, vals):
    self.name = name
    self.vals = vals
  def toArray(self):
    return ["--%s=%s" % (self.name, val) for val in self.vals]


# Set up OptionSets. Note that giant cross product is done over all JavaOptionsSets + OptionSets
# passed to each test which may be combinations of those set up here.

# First set up Java options.
common_java_opts = {
  # Fraction of JVM memory used for caching RDDs
  JavaOptionSet("spark.storage.memoryFraction", [0.66]),
  JavaOptionSet("spark.serializer", ["spark.JavaSerializer"]),
  JavaOptionSet("spark.executor.memory", ["9g"])
}

# The following options value sets are shared among all tests.
common_opts = [
  # How many times to run each experiment - used to warm up system caches.
  # This OptionSet should probably only have a single value (i.e., length 1)
  # since it doesn't make sense to have multiple values here.
  OptionSet("num-trials", [5]),
  # The number of input partitions.
  OptionSet("num-partitions", [3]),
  # The number of reduce tasks.
  OptionSet("reduce-tasks", [400]),
  # A random seed to make tests reproducable.
  OptionSet("random-seed", [5]),
  # input persistence strategyi (can be "memory" or "disk").
  OptionSet("persistent-type", ["memory"])
]

# The following options value sets are shared among all tests of
# operations on key-value data.
assert scale_factor > 0 and scale_factor <= 1.0, "scale_factor must be > 0 and <= 10."
key_val_test_opts = [
  OptionSet("num-records",   [max(1,int(200 * 1000 * 1000 * scale_factor))]),
  OptionSet("unique-keys",   [max(1, int(20 * 1000 * scale_factor))]),
  OptionSet("key-length",    [10]),
  OptionSet("unique-values", [max(1, int(1000 * 1000 * scale_factor))]),
  OptionSet("value-length",  [10])
]


# Test setup
# ----------------------------------------------------------------------------------------
# Set up the actual tests. Each test is represtented by a key val pair
# where key = short_name, test_cmd, and val = list<OptionSet>.
# @param option_space is a list with elements of type OptionSet or list of args
kv_opts = common_opts + key_val_test_opts
TESTS = []

TESTS += [("scala-agg-by-key", "spark.perf.TestRunner aggregate-by-key", common_java_opts, kv_opts)]

TESTS += [("scala-sort-by-key", "spark.perf.TestRunner sort-by-key", common_java_opts, kv_opts)]

TESTS += [("scala-count", "spark.perf.TestRunner count", common_java_opts, kv_opts)]

TESTS += [("scala-count-w-fltr", "spark.perf.TestRunner count-with-filter",
    common_java_opts, kv_opts)]


# Advanced Configuration Options
# ----------------------------------------------------------------------------------------

# Skip downloading and building Spark (requires Spark already be built in the spark directory).
SKIP_SPARK_PREP = False

# Skip building and packaging tests (requires perf-tests already be packaged in the target 
# directory).
SKIP_TEST_PREP = False

# Skip warming up local disks
SKIP_DISK_WARMUP = False
