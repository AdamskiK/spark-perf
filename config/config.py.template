# Variables in ALL_CAPS are used by the testing framework,
# while variables in lower_case are used locally to set up
# other variables.

# Spark Configuration Options
# --------------------------------------------------

SPARK_MASTER = "local"

# After building a new Spark, conf files (e.g. spark-env.sh and slaves file)
# will be copied in from this directory.
SPARK_CONF_DIR = "/home/pwendell/Documents/spark/conf"

# Java options. 
# NOTE: This will be overridden if you define SPARK_JAVA_OPTS in spark-env.sh  
JAVA_OPTS = {
  # Fraction of JVM memory used for caching RDDs
  "spark.storage.memoryFraction": 0.66,
  # Use coarse-grained mesos scheduling
  "spark.mesos.coarse": False,
  # Type of serialization used: spark.JavaSerializer, spark.KryoSerializer
  "spark.serializer": "spark.JavaSerializer",
  # Enable/Disable Compression
  "spark.blockManager.compress": False,
  # Temp file directory
  "spark.local.dir": "/mnt/tmp"
}

# Test Configuration
# --------------------------------------------------

# Represents an option and a set of values for that option
# that we will sweep over in a test run.
class OptionSet:
  def __init__(self, name, vals):
    self.name = name
    self.vals = vals
  def toArray(self):
    return ["--%s=%s" % (self.name, val) for val in self.vals]


# Set up OptionSets.
# Note that giant cross product is done over all OptionSets passed to each
# which may be combinations of those set up here.
# --------------------------------------------------------

# The following options value sets are shared among all tests.
common_args = [
  # How many times to run each experiment - used to warm up system caches.
  OptionSet("num-trials", [2]),
  # The number of input partitions.
  OptionSet("num-partitions", [3]),
  # The number of reduce tasks.
  OptionSet("reduce-tasks", [4]),
  # A random seed to make tests reproducable.
  OptionSet("random-seed", [5]),
  # input persistence strategyi (can be "memory" or "disk").
  OptionSet("persistent-type", ["memory"])
]

# The following options value sets are shared among all tests of
# operations on key-value data.
key_val_test_args = [
  OptionSet("num-records",   [6,60]),
  OptionSet("unique-keys",   [7]),
  OptionSet("key-length",    [8]),
  OptionSet("unique-values", [9]),
  OptionSet("value-length",  [10])
]

# Set up the actual tests. Each test is represtented by a key val pair
# where key = short_name, test_cmd, and val = list<OptionSet>.
# @param option_space is a list with elements of type OptionSet or list of args
# --------------------------------------------------
scala_tests_base_cmd = "spark.perf.TestRunner %s" % SPARK_MASTER
kv_args = common_args + key_val_test_args
TESTS = [
  ("scala-agg-by-key", "%s aggregate-by-key" % scala_tests_base_cmd, kv_args),
  ("scala-sort-by-key", "%s sort-by-key" % scala_tests_base_cmd, kv_args),
  ("scala-count", "%s count" % scala_tests_base_cmd, kv_args),
  ("scala-count-w-fltr", "%s count-with-filter" % scala_tests_base_cmd, kv_args)
]
