import time

# Variables in ALL_CAPS are used by the testing framework,
# while variables in lower_case are used locally to set up
# other variables.

# Standard Configuration Options
# --------------------------------------------------

# Git commit id to checkout and test. The repository location configured via SPARK_GIT_REPO is 
# named "origin". Other than a hash code, you can also provide a branch name, or to test a pull 
# request, give a string of the format origin/pr/N to checkout pull request N.
COMMIT_ID = ""

# File to write results to
OUTPUT_FILENAME = "spark_perf_output_%s" % time.strftime("%Y-%m-%d_%H-%M-%S") 

SPARK_GIT_REPO = "git://github.com/mesos/spark.git"

# The test framework clones its own clean copy of Spark, and then
# the files in this dir (e.g. spark-env.sh and slaves file)
# are copied from this directory to that Spark's conf dir
# before we start the Spark. To test on your local machine,
# create a spark-env.sh and a slaves file with a single slave set
# as your local machine.
SPARK_CONF_DIR = "/root/spark/conf"

# This default setting assumes we are running on the Spark EC2 AMI. Developers will probably want
# to change this to SPARK_CLUSTER_URL = "spark://localhost:7077" for testing.
SPARK_CLUSTER_URL = open("/root/spark-ec2/cluster-url", 'r').readline().strip()

# Command used to launch Scala
SCALA_CMD = "scala"

# Test Configuration
# --------------------------------------------------

# Represents a Java option and a set of values for it that we will sweep over in a test run.
class JavaOptionSet:
  def __init__(self, name, vals):
    self.name = name
    self.vals = vals
  def to_array(self):
    return ["-D%s=%s" % (self.name, val) for val in self.vals]

# Represents an option and a set of values for that option that we will sweep over in a test run.
class OptionSet:
  def __init__(self, name, vals):
    self.name = name
    self.vals = vals
  def to_array(self):
    return ["--%s=%s" % (self.name, val) for val in self.vals]

# Represents a flag-style option and a set of values that
# we will sweep over in a test run. Values can be True or False.
class FlagSet:
  def __init__(self, name, vals):
    self.name = name
    self.vals = vals
  def toArray(self):
    for val in self.vals:
      assert val == True or val == False, "FlagSet value for %s is not True or False" % self.name
    return ["--%s" % self.name if val is True else "" for val in self.vals]


# Set up OptionSets.
# Note that giant cross product is done over all JavaOptionsSets + OptionSets passed to each test
# which may be combinations of those set up here.
# --------------------------------------------------------

# Java options.
common_java_opts = {
  # Fraction of JVM memory used for caching RDDs
  JavaOptionSet("spark.storage.memoryFraction", [0.66]),
  # Type of serialization used: spark.JavaSerializer, spark.KryoSerializer
  JavaOptionSet("spark.serializer", ["spark.JavaSerializer"])
}

# The following options value sets are shared among all tests.
common_opts = [
  # How many times to run each experiment - used to warm up system caches.
  # This OptionSet should probably only have a single value (i.e., length 1)
  # since it doesn't make sense to have multiple values here.
  OptionSet("num-trials", [2]),
  # The number of input partitions.
  OptionSet("num-partitions", [3]),
  # The number of reduce tasks.
  OptionSet("reduce-tasks", [4]),
  # A random seed to make tests reproducable.
  OptionSet("random-seed", [5]),
  # input persistence strategyi (can be "memory" or "disk").
  OptionSet("persistent-type", ["memory"]),
  # whether to wait for input in order to exit the JVM
  FlagSet("wait-for-exit", [False])
]

# The following options value sets are shared among all tests of
# operations on key-value data.
key_val_test_opts = [
  OptionSet("num-records",   [100]),
  OptionSet("unique-keys",   [7]),
  OptionSet("key-length",    [8]),
  OptionSet("unique-values", [9]),
  OptionSet("value-length",  [10])
]

# Set up the actual tests. Each test is represtented by a key val pair
# where key = short_name, test_cmd, and val = list<OptionSet>.
# @param option_space is a list with elements of type OptionSet or list of args
# --------------------------------------------------
kv_opts = common_opts + key_val_test_opts
TESTS = [
  ("scala-agg-by-key", "spark.perf.TestRunner aggregate-by-key", common_java_opts, kv_opts),
  ("scala-sort-by-key", "spark.perf.TestRunner sort-by-key", common_java_opts, kv_opts),
  ("scala-count", "spark.perf.TestRunner count", common_java_opts, kv_opts),
  ("scala-count-w-fltr", "spark.perf.TestRunner count-with-filter", common_java_opts, kv_opts)
]

# Advanced Configuration Options
# --------------------------------------------------

# Skip downloading and building Spark (requires Spark already be built in the spark directory).
SKIP_SPARK_PREP = False

# Skip building and packaging tests (requires perf-tests already be packaged in the target 
# directory).
SKIP_TEST_PREP = False

# Skip warming up local disks
SKIP_DISK_WARMUP = False

# Total number of bytes used to warm up each local directory.
DISK_WARMUP_BYTES = 200 * 1024 * 1024

# Number of files to create when warming up each local directory. Bytes will be evenly divided 
# across files.
DISK_WARMUP_FILES = 200

