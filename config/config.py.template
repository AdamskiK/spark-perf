# Variables in ALL_CAPS are used by the testing framework,
# while variables in lower_case are used locally to set up
# other variables.

# Spark Configuration Options
# --------------------------------------------------

SPARK_GIT_REPO = "git://github.com/mesos/spark.git"

# The test framework clones its own clean copy of Spark, and then
# the files in this dir (e.g. spark-env.sh and slaves file)
# are copied from this directory to that Spark's conf dir
# before we start the Spark. To test on your local machine,
# create a spark-env.sh and a slaves file with a single slave set
# as your local machine.
SPARK_CONF_DIR = "/root/spark/conf"

# This default setting assumes we are running on the Spark EC2 AMI. Developers will probably want
# to change this to SPARK_CLUSTER_URL = "spark://localhost:7077" for testing.
SPARK_CLUSTER_URL = open("/root/spark-ec2/cluster-url", 'r').readline().strip()

# Java options. 
# NOTE: This will be overridden if you define SPARK_JAVA_OPTS in spark-env.sh  
JAVA_OPTS = {
  # Fraction of JVM memory used for caching RDDs
  "spark.storage.memoryFraction": 0.66,
  # Type of serialization used: spark.JavaSerializer, spark.KryoSerializer
  "spark.serializer": "spark.JavaSerializer",
}

# Command used to launch Scala
SCALA_CMD = "scala"

# Test Configuration
# --------------------------------------------------

# Represents an option and a set of values for that option
# that we will sweep over in a test run.
class OptionSet:
  def __init__(self, name, vals):
    self.name = name
    self.vals = vals
  def toArray(self):
    return ["--%s=%s" % (self.name, val) for val in self.vals]


# Set up OptionSets.
# Note that giant cross product is done over all OptionSets passed to each
# which may be combinations of those set up here.
# --------------------------------------------------------

# The following options value sets are shared among all tests.
common_args = [
  # How many times to run each experiment - used to warm up system caches.
  # This OptionSet should probably only have a single value (i.e., length 1)
  # since it doesn't make sense to have multiple values here.
  OptionSet("num-trials", [2]),
  # The number of input partitions.
  OptionSet("num-partitions", [3]),
  # The number of reduce tasks.
  OptionSet("reduce-tasks", [4]),
  # A random seed to make tests reproducable.
  OptionSet("random-seed", [5]),
  # input persistence strategyi (can be "memory" or "disk").
  OptionSet("persistent-type", ["memory"])
]

# The following options value sets are shared among all tests of
# operations on key-value data.
key_val_test_args = [
  OptionSet("num-records",   [100]),
  OptionSet("unique-keys",   [7]),
  OptionSet("key-length",    [8]),
  OptionSet("unique-values", [9]),
  OptionSet("value-length",  [10])
]

# Set up the actual tests. Each test is represtented by a key val pair
# where key = short_name, test_cmd, and val = list<OptionSet>.
# @param option_space is a list with elements of type OptionSet or list of args
# --------------------------------------------------
kv_args = common_args + key_val_test_args
TESTS = [
  ("scala-agg-by-key", "spark.perf.TestRunner aggregate-by-key", kv_args),
  ("scala-sort-by-key", "spark.perf.TestRunner sort-by-key", kv_args),
  ("scala-count", "spark.perf.TestRunner count", kv_args),
  ("scala-count-w-fltr", "spark.perf.TestRunner count-with-filter", kv_args)
]
