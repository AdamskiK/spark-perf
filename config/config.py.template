"""Configuration options for running the Spark performance tests (perf-tests)."""

import time

from config_utils import FlagSet
from config_utils import JavaOptionSet
from config_utils import OptionSet

# Standard Configuration Options
# ----------------------------------------------------------------------------------------

# Git commit id to checkout and test. The repository location configured via SPARK_GIT_REPO is
# named "origin". This ID can specify any of the following:
#     1. A git commit hash     e.g. "4af93ff3"
#     2. A branch name             e.g. "origin/branch-0.7"
#     3. A pull request            e.g. "origin/pr/675"
COMMIT_ID = ""

# Whether to merge this commit into master before testing. If the commit cannot be merged into
# master, the script will exit with a failure.
MERGE_INTO_MASTER = False

# File to write results to.
OUTPUT_FILENAME = "spark_perf_output_%s_%s" % (
    COMMIT_ID.replace("/", "-"), time.strftime("%Y-%m-%d_%H-%M-%S"))

SPARK_GIT_REPO = "git://github.com/mesos/spark.git"

# The test framework clones its own clean copy of Spark, and then the files in this dir (e.g.
# spark-env.sh and slaves file) are copied from this directory to that Spark's conf dir before we
# start the Spark. To test on your local machine, create a spark-env.sh and a slaves file with a
# single slave set as your local machine.
SPARK_CONF_DIR = "/root/spark/conf"

# This default setting assumes we are running on the Spark EC2 AMI. Developers will probably want
# to change this to SPARK_CLUSTER_URL = "spark://localhost:7077" for testing.
SPARK_CLUSTER_URL = open("/root/spark-ec2/cluster-url", 'r').readline().strip()

# Command used to launch Scala.
SCALA_CMD = "scala"

# The default values configured below are appropriate for approximately 20 m1.xlarge nodes.
# Use this variable to scale the values if you are running the tests with more or fewer nodes.
SCALE_FACTOR = 1.0

assert SCALE_FACTOR > 0, "SCALE_FACTOR must be > 0."


# Test Configuration
# ----------------------------------------------------------------------------------------

# Set up OptionSets. Note that giant cross product is done over all JavaOptionsSets + OptionSets
# passed to each test which may be combinations of those set up here.

# Java options.
COMMON_JAVA_OPTS = [
    # Fraction of JVM memory used for caching RDDs.
    JavaOptionSet("spark.storage.memoryFraction", [0.66]),
    JavaOptionSet("spark.serializer", ["spark.JavaSerializer"]),
    JavaOptionSet("spark.executor.memory", ["9g"]),
    # To ensure consistency across runs, we disable delay scheduling
    JavaOptionSet("spark.locality.wait", [str(60 * 1000 * 1000)])
]

# The following options value sets are shared among all tests.
COMMON_OPTS = [
    # How many times to run each experiment - used to warm up system caches.
    # This OptionSet should probably only have a single value (i.e., length 1)
    # since it doesn't make sense to have multiple values here.
    OptionSet("num-trials", [5]),
    # Extra pause added between trials, in seconds
    OptionSet("inter-trial-wait", [30])
]

# The following options value sets are shared among all tests of
# operations on key-value data.
KEY_VAL_TEST_OPTS = [
    # The number of input partitions.
    OptionSet("num-partitions", [400], True),
    # The number of reduce tasks.
    OptionSet("reduce-tasks", [400], True),
    # A random seed to make tests reproducable.
    OptionSet("random-seed", [5]),
    # Input persistence strategy (can be "memory" or "disk").
    OptionSet("persistent-type", ["memory"]),
    # Whether to wait for input in order to exit the JVM.
    FlagSet("wait-for-exit", [False]),
    # Total number of records to create.  
    OptionSet("num-records", [200 * 1000 * 1000], True),
    # Number of unique keys to sample from.
    OptionSet("unique-keys",[20 * 1000], True),
    # Length in characters of each key.
    OptionSet("key-length", [10]),
    # Number of unique values to sample from.
    OptionSet("unique-values", [1000 * 1000], True),
    # Length in characters of each value.
    OptionSet("value-length", [10])
]


# Test setup
# ----------------------------------------------------------------------------------------
# Set up the actual tests. Each test is represtented by a tuple: (short_name, test_cmd,
# scale_factor, list<JavaOptionSet>, list<OptionSet>).

KV_OPTS = COMMON_OPTS + KEY_VAL_TEST_OPTS

TESTS = []

TESTS += [("scheduling-tput", "spark.perf.TestRunner scheduling-throughput", SCALE_FACTOR,
  COMMON_JAVA_OPTS, [OptionSet("num-tasks", [10 * 1000])] + COMMON_OPTS)]

TESTS += [("scala-agg-by-key", "spark.perf.TestRunner aggregate-by-key", SCALE_FACTOR,
    COMMON_JAVA_OPTS, KV_OPTS)]

# Scale the input for this test by an additional 0.10.
TESTS += [("scala-sort-by-key", "spark.perf.TestRunner sort-by-key", SCALE_FACTOR * 0.1,
    COMMON_JAVA_OPTS, KV_OPTS)]

TESTS += [("scala-count", "spark.perf.TestRunner count", SCALE_FACTOR, COMMON_JAVA_OPTS, KV_OPTS)]

TESTS += [("scala-count-w-fltr", "spark.perf.TestRunner count-with-filter", SCALE_FACTOR,
    COMMON_JAVA_OPTS, KV_OPTS)]


# Advanced Configuration Options
# ----------------------------------------------------------------------------------------

# Skip downloading and building Spark (requires Spark already be built in the spark directory).
SKIP_SPARK_PREP = False

# Skip building and packaging tests (requires perf-tests already be packaged in the target
# directory).
SKIP_TEST_PREP = False

# Skip warming up local disks.
SKIP_DISK_WARMUP = False

# Total number of bytes used to warm up each local directory.
DISK_WARMUP_BYTES = 200 * 1024 * 1024

# Number of files to create when warming up each local directory. Bytes will be evenly divided
# across files.
DISK_WARMUP_FILES = 200

# Prompt for confirmation when deleting temporary files.
PROMPT_FOR_DELETES = True
