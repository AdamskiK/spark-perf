#!/usr/bin/env python

import itertools
import os.path
import sys
from subprocess import Popen, PIPE

if len(sys.argv) != 3:
  sys.exit("Usage: ./run [Git Commit Hash] [Output Filename]")

proj_dir = os.path.dirname(os.path.abspath(__file__)) + "/.." 
sbt_cmd = "sbt/sbt"

# Run shell command and ignore output
def run_cmd(cmd):
  Popen(cmd, stdout=sys.stderr, shell=True).wait()

# Check if a config file exists
if not os.path.isfile(proj_dir+"/config/config.py"):
  sys.exit("Please create a config file called %s/config/config.py" % proj_dir)

# import the configuration settings
sys.path.append(proj_dir + "/config")
import config

print("Downloading Spark...\n")
# Clone the Git repository and create a branch for the hash.
if not os.path.isdir("spark"):
  run_cmd("git clone git://github.com/mesos/spark.git")
# Pull just in case spark was already cloned earlier.
else:
  run_cmd("cd spark; git pull origin master:master")

# Build Spark
print("Building branch %s. This may take a while...\n" % sys.argv[1])
run_cmd("cd spark; git reset --hard %s; %s package" % (sys.argv[1], sbt_cmd))

# Copy Spark configuration files to new directory
print("Copying configuration files to Spark")
if not os.path.exists(config.SPARK_CONF_DIR):
  sys.exit("Could not find Spark config directory: %s" % config.SPARK_CONF_DIR)
run_cmd("cp %s/* %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))

# Building the perf-tests code against the downloaded version of Spark.
print("Building spark-perf...\n")
run_cmd("cd %s; %s package" % (proj_dir, sbt_cmd))

# Sync the whole directory to the slaves. 
# Here we are assuming we are using our Amazon EC2 AMI, fixing this is a TODO.
run_cmd("%s/spark-ec2/copy-dir %s" % (proj_dir, proj_dir))
print("%s/spark-ec2/copy-dir not found. Continuing without syncing to slaves..." % proj_dir)


# Set Spark Java Options (from config.py)
# --------------------------------------------------
new_env = os.environ.copy()

new_env["SPARK_JAVA_OPTS"] = ""
for k, v in config.JAVA_OPTS.iteritems():
  new_env["SPARK_JAVA_OPTS"] += "-D%s=%s " % (k, v)

# Clear the Spark local directories.
def clear_dir (dir_name):
  print("Clearing dir %s.\n" % dir_name)
  run_cmd("rm -r %s/*" % dir_name)
  run_cmd("%s/mesos-ec2/copy-dir %s" % (dir_name, dir_name))

map(clear_dir, config.JAVA_OPTS["spark.local.dir"].split(","))

def average(in_list):
  return sum(in_list) / len(in_list)

def variance(in_list):
  variance = 0
  for x in in_list:
    variance = variance + (average(in_list) - x) ** 2
  return variance / len(in_list)

# Run all tests set up in the Config file.
# --------------------------------------------------
if "spark://" in config.SPARK_MASTER:
  prinln("Restarting standalone scheduler...")
  run_cmd("%s/spark/bin/stop-all.sh")
  run_cmd("%s/spark/bin/start-all.sh")

out_file = open(sys.argv[2], 'w')
for short_name, test_cmd, option_sets in config.TESTS:
  print("\n---------------------------------------------")
  print("Running all tests for %s.\n" % test_cmd)
  # Run a test for all combinations of the OptionSets given, then capture
  # and print the output.
  option_set_arrays = [i.toArray() for i in option_sets]
  for opt_list in itertools.product(*option_set_arrays):
    results_token = "results: "
    # TODO(andy): Add a timout on the subprocess.
    cmd = '%s "run %s %s"' % (sbt_cmd, test_cmd, " ".join(opt_list))
    print("\nrunning command: %s\n" % cmd)
    output = Popen(cmd, stdout=PIPE, shell=True, env=new_env).stdout.read()
    if results_token not in output:
      print("Test did not produce expected results. Output was:")
      print(output)
      sys.exit(1)
    result_line = filter(lambda x: results_token in x, output.split("\n"))[0]
    result_list = result_line.replace(results_token, "").split(",")
    # TODO(andy): For even cardinality lists, return average of middle two elts.
    result_list = sorted([float(x) for x in result_list])
    result_med = result_list[len(result_list)/2]
    result_var = variance(result_list)
    result_string = "%s, %s, %s, %s" % (short_name, " ".join(opt_list), result_med, result_var)
    print(result_string)
    out_file.write(result_string + "\n")
    sys.stdout.flush()
    out_file.flush()

print("Finished running all tests. See CSV output in %s" % sys.argv[2])
