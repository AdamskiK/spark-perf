#!/usr/bin/env python

import argparse
import re
import itertools
import os.path
import sys
from subprocess import Popen, PIPE

proj_dir = os.path.dirname(os.path.abspath(__file__)) + "/.." 
sbt_cmd = "sbt/sbt"

parser = argparse.ArgumentParser(description='Run Spark Peformance Tests.')

parser.add_argument('commit_id', help='git commit id to checkout and test')
parser.add_argument('output_filename', help='file to write results in')
parser.add_argument('--config-file', help='override default location of config file, must be a python file that ends in .py', default=proj_dir+"/config/config.py")
parser.add_argument('--skip-spark-prep', help='skip downloading and building Spark (requires Spark already be built in the spark directory)', action='store_true')

args = parser.parse_args()
assert args.config_file.endswith(".py"), "config filename must end with .py"

# Run shell command and ignore output
def run_cmd(cmd):
  print(cmd)
  Popen(cmd, stdout=sys.stderr, shell=True).wait()

def copy_dir_to_slaves(dir_name, slaves):
  for slave in slaves:
    print("rsync'ing %s to slave %s" % (dir_name, slave))
    run_cmd('rsync --delete -e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5" -az "%s" "%s:%s" & sleep 0.5' % (dir_name, slave.strip(), os.path.abspath(dir_name)))

# Check if the config file exists
assert os.path.isfile(args.config_file), "Please create a config file called %s (you probably just want to copy and then modify %s/config/config.py.template)" % (args.config_file, proj_dir)

# import the configuration settings from the config file
print("Adding %s to path" % os.path.dirname(args.config_file))
sys.path.append(os.path.dirname(args.config_file))
print("running import %s" % os.path.basename(args.config_file).split(".")[0])
exec("import %s" % os.path.basename(args.config_file).split(".")[0])

if args.skip_spark_prep:
  print("Skipping prepartion tasks for Spark (i.e, git clone or update, checkout, build, copy conf files)")
else:
  # Clone or fetch updates for the spark repository.
  if not os.path.isdir("spark"):
    print("Git cloning Spark...")
    run_cmd("git clone %s" % config.SPARK_GIT_REPO)
  else:
    print("Updating Spark repo...")
    run_cmd("cd spark; git fetch origin")

  # Build Spark
  print("Cleaning Spark and building branch %s. This may take a while...\n" % args.commit_id)
  run_cmd("cd spark; git clean -f -d -x; git reset --hard %s; %s clean package" % (args.commit_id, sbt_cmd))

  # Copy Spark configuration files to new directory
  print("Copying all files from %s to %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))
  assert os.path.exists("%s/spark-env.sh" % config.SPARK_CONF_DIR), \
         "Could not find required file %s/spark-env.sh" % config.SPARK_CONF_DIR
  assert os.path.exists("%s/slaves" % config.SPARK_CONF_DIR), \
         "Could not find required file %s/slaves" % config.SPARK_CONF_DIR
  run_cmd("cp %s/* %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))

# Building the perf-tests code against the downloaded version of Spark.
print("Building perf-tests...")
run_cmd("cd %s; %s package" % (proj_dir, sbt_cmd))

# Sync the whole directory to the slaves. 
print("Syncing the test directory to the slaves.")
slaves_file_raw = open("%s/slaves" % config.SPARK_CONF_DIR, 'r').read().split("\n")
slaves_list = filter(lambda x: not x.startswith("#"), slaves_file_raw)
copy_dir_to_slaves(proj_dir, slaves_list)

# Set Spark Java Options (from config.py)
new_env = os.environ.copy()

new_env["SPARK_JAVA_OPTS"] = ""
for k, v in config.JAVA_OPTS.iteritems():
  new_env["SPARK_JAVA_OPTS"] += "-D%s=%s " % (k, v)
new_env["SPARK_HOME"] = "%s/spark" % proj_dir

# Clear the Spark local directories.
def clear_dir (dir_name):
  print("Clearing spark local dir %s." % dir_name)
  run_cmd("rm -r %s/*" % dir_name)
  run_cmd("%s/spark-ec2/copy-dir %s" % (proj_dir, dir_name))

spark_env = open("%s/spark-env.sh" % config.SPARK_CONF_DIR, 'r').read() 
re_result = re.search(r'spark.local.dir=([^"]*)"', spark_env)
if re_result:
  spark_local_dirs = re_result.group(1).split(",")
  map(clear_dir, spark_local_dirs)
else:
  sys.exit("These scripts require you to explicitly set spark.local.dir in spark-env.sh so that "
           "it can be cleaned. The way we check this is pretty picky, specifically we try to "
           "find the following string in spark-env.sh: spark.local.dir=ONE_OR_MORE_DIRNAMES\"")

# Try shutting down the cluster started by the Spark EC2 scripts,
# just in case it (exists and) is still running.
if os.path.exists("/root/spark/bin/stop-all.sh"):
  run_cmd("/root/spark/bin/stop-all.sh" % proj_dir)

# Restart our test Spark cluster.
print("Restarting standalone scheduler...")
run_cmd("%s/spark/bin/stop-all.sh" % proj_dir)
run_cmd("%s/spark/bin/start-all.sh" % proj_dir)

# Some utility functions to calculate useful stats on test output.
def average(in_list):
  return sum(in_list) / len(in_list)

def variance(in_list):
  variance = 0
  for x in in_list:
    variance = variance + (average(in_list) - x) ** 2
  return variance / len(in_list)

# Run all tests specified in the Config file.
out_file = open(args.output_filename, 'w')
for short_name, test_cmd, option_sets in config.TESTS:
  print("\n---------------------------------------------")
  print("Running all tests for %s.\n" % test_cmd)
  # Run a test for all combinations of the OptionSets given, then capture
  # and print the output.
  option_set_arrays = [i.toArray() for i in option_sets]
  for opt_list in itertools.product(*option_set_arrays):
    results_token = "results: "
    # TODO(andy): Add a timout on the subprocess.
    cmd = '%s "run %s %s"' % (sbt_cmd, test_cmd, " ".join(opt_list))
    print("\nrunning command: %s\n" % cmd)
    output = Popen(cmd, stdout=PIPE, shell=True, env=new_env).stdout.read()
    if results_token not in output:
      print("Test did not produce expected results. Output was:")
      print(output)
      sys.exit(1)
    result_line = filter(lambda x: results_token in x, output.split("\n"))[0]
    result_list = result_line.replace(results_token, "").split(",")
    # TODO(andy): For even cardinality lists, return average of middle two elts.
    result_list = sorted([float(x) for x in result_list])
    result_med = result_list[len(result_list)/2]
    result_var = variance(result_list)
    result_first = result_list[0]
    result_last = result_list[len(result_list) - 1]
    result_string = "%s, %s, %s, %s, %s, %s" % (short_name, " ".join(opt_list), result_med, result_var, result_first, result_last)
    print(result_string)
    out_file.write(result_string + "\n")
    sys.stdout.flush()
    out_file.flush()

print("Finished running all tests. See CSV output in %s" % args.output_filename)
