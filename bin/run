#!/usr/bin/env python

import argparse
import itertools
from math import sqrt
import os.path
import re
from subprocess import Popen, PIPE
import sys
import time

proj_dir = os.path.dirname(os.path.abspath(__file__)) + "/.." 
sbt_cmd = "sbt/sbt"

parser = argparse.ArgumentParser(description='Run Spark peformance tests. Before running, edit the supplied configuration file.')

parser.add_argument('--config-file', help='override default location of config file, must be a '
    'python file that ends in .py', default="%s/config/config.py" % proj_dir)

args = parser.parse_args()
assert args.config_file.endswith(".py"), "config filename must end with .py"

# Check if the config file exists.
assert os.path.isfile(args.config_file), ("Please create a config file called %s (you probably "
    "just want to copy and then modify %s/config/config.py.template)" % 
    (args.config_file, proj_dir))

# Import the configuration settings from the config file.
print("Adding %s to path" % os.path.dirname(args.config_file))
sys.path.append(os.path.dirname(args.config_file))
print("running import %s" % os.path.basename(args.config_file).split(".")[0])
exec("import %s" % os.path.basename(args.config_file).split(".")[0])
assert config.COMMIT_ID is not "", "Please specify COMMIT_ID in %s" % args.config_file

# Run shell command and ignore output.
def run_cmd(cmd, exit_on_fail=True):
  if cmd.find(";") != -1:
    print("***************************")
    print("WARNING: the following command contains a semicolon which may cause non-zero return "
          "values to be ignored. This isn't necessarily a problem, but proceed with caution!")
  print(cmd)
  return_code = Popen(cmd, stdout=sys.stderr, shell=True).wait()
  if exit_on_fail:
    assert return_code == 0, ("The following shell command finished with a non-zero returnode (%s):"
        " %s") % (return_code, cmd)

# Copy a directory to a list of slaves. Provides similar functionality to
# spark-ec2/copy-dir (this is a port of that actually).
def copy_dir_to_slaves(dir_name, slaves):
  for slave in slaves:
    print("rsync'ing %s to slave %s" % (dir_name, slave))
    run_cmd('rsync --delete -e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5" -az "%s"'
        '"%s:%s" & sleep 0.5' % (dir_name, slave.strip(), os.path.abspath(dir_name)))


# Run the supplied command on each slave.
def run_cmd_on_slaves(cmd_name, slaves):
  for slave in slaves:
    print("Running command on slave %s." % slave)
    run_cmd("ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 %s '%s'" % (slave, cmd_name))

# Try shutting down the cluster started by the Spark EC2 scripts,
# just in case it (exists and) is still running.
if os.path.exists("/root/spark/bin/stop-all.sh"):
  run_cmd("/root/spark/bin/stop-all.sh")

# Stop our Spark cluster, if it's running.
if os.path.exists("%s/spark/bin/stop-all.sh" % proj_dir):
  print("Stopping Spark standalone cluster...")
  run_cmd("%s/spark/bin/stop-all.sh" % proj_dir)
  time.sleep(2) # Killing the cluster takes a little time so give it a second.

# Prep Spark (unless arg was passed that causes this to be skipped).
if config.SKIP_SPARK_PREP:
  # TODO(andy): Make this check for the jar we will be including on the
  #             classpath (as part of work Patrick is doing), instead of
  #             just looking for spark/target.
  assert os.path.exists("%s/spark/target" % proj_dir), ("You chose to skip Spark prep, but we "
      "can't since %s/spark does not exist, so Spark needs to be rebuilt/packaged.") % proj_dir
  print("Skipping prepartion tasks for Spark (i.e, git clone or update, checkout, build, copy "
      "conf files)")
else:
  # Clone or fetch updates for the spark repository.
  if not os.path.isdir("spark"):
    print("Git cloning Spark...")
    run_cmd("git clone %s spark" % config.SPARK_GIT_REPO)
    run_cmd("cd spark; git config --add remote.origin.fetch "
        "'+refs/pull/*/head:refs/remotes/origin/pr/*'")

  os.chdir("spark")
  print("Updating Spark repo...")
  run_cmd("git fetch")

  # Build Spark.
  print("Cleaning Spark and building branch %s. This may take a while...\n" % config.COMMIT_ID)
  run_cmd("git clean -f -d -x")
  run_cmd("git reset --hard %s" % config.COMMIT_ID)
  run_cmd("%s clean package" % sbt_cmd)

  # Copy Spark configuration files to new directory.
  print("Copying all files from %s to %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))
  assert os.path.exists("%s/spark-env.sh" % config.SPARK_CONF_DIR), \
         "Could not find required file %s/spark-env.sh" % config.SPARK_CONF_DIR
  assert os.path.exists("%s/slaves" % config.SPARK_CONF_DIR), \
         "Could not find required file %s/slaves" % config.SPARK_CONF_DIR
  run_cmd("cp %s/* %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))

  os.chdir("..")

# Start our Spark cluster.
print("Starting a Spark standalone cluster to use for testing...")
assert os.path.exists("%s/spark/bin/start-all.sh" % proj_dir), ("%s/spark/bin/start-all.sh must "
    "exist") % proj_dir
run_cmd("%s/spark/bin/start-all.sh" % proj_dir)
time.sleep(2) # Starting the cluster takes a little time so give it a second.

# Building the perf-tests code against the downloaded version of Spark.
print("Building perf-tests...")
if config.SKIP_TEST_PREP:
  test_jar = "%s/target/perf-tests-assembly.jar" % proj_dir
  assert os.path.exists(test_jar), ("You tried to skip packaging the perf tests, but %s was not "
      "already present") % test_jar
else:
  run_cmd("cd %s; %s assembly" % (proj_dir, sbt_cmd))

# Sync the whole directory to the slaves. 
print("Syncing the test directory to the slaves.")
slaves_file_raw = open("%s/slaves" % config.SPARK_CONF_DIR, 'r').read().split("\n")
slaves_list = filter(lambda x: not x.startswith("#"), slaves_file_raw)
copy_dir_to_slaves(proj_dir, slaves_list)

# Set Spark Java Options (from config.py)
new_env = os.environ.copy()

new_env["SPARK_HOME"] = "%s/spark" % proj_dir

# Clear the Spark local directories.
def clear_dir (dir_name):
  print("Clearing spark local dir %s." % dir_name)
  run_cmd("rm -r %s/*" % dir_name, False)
  copy_dir_to_slaves(dir_name, slaves_list)

spark_env = open("%s/spark-env.sh" % config.SPARK_CONF_DIR, 'r').read() 
re_result = re.search(r'spark.local.dir=([^"]*)"', spark_env)
if re_result:
  spark_local_dirs = re_result.group(1).split(",")
  map(clear_dir, spark_local_dirs)
else:
  sys.exit("ERROR: These scripts require you to explicitly set spark.local.dir in spark-env.sh"
      "so that it can be cleaned. The way we check this is pretty picky, specifically we try to "
      "find the following string in spark-env.sh: spark.local.dir=ONE_OR_MORE_DIRNAMES\" so you "
      "will want a line like this: SPARK_JAVA_OPTS+=\" -Dspark.local.dir=/tmp\"")

if not config.SKIP_DISK_WARMUP:
  for d in spark_local_dirs:
    # TODO(pwendell): Make these configurable and set in the conf file once we migrate options there
    bytes_to_write = 1000 * 1024 * 1024
    bytes_per_file = 5 * 1024 * 1024
    command = "dd if=/dev/urandom bs=%s count=1 | split -a 5 -b %s - %s/random" % (
        bytes_to_write, bytes_per_file, d)
    print("Generating test data for %s, this may take some time" % d)
    run_cmd(command)
    run_cmd_on_slaves(command, slaves_list)

# Some utility functions to calculate useful stats on test output.
def average(in_list):
  return sum(in_list) / len(in_list)

def variance(in_list):
  variance = 0
  for x in in_list:
    variance = variance + (average(in_list) - x) ** 2
  return variance / len(in_list)

# Run all tests specified in the Config file.
out_file = open(config.OUTPUT_FILENAME, 'w')
for short_name, test_cmd, java_opt_sets, opt_sets in config.TESTS:
  print("\n---------------------------------------------")
  print("Running all tests for %s.\n" % test_cmd)
  # Run a test for all combinations of the OptionSets given, then capture
  # and print the output.
  java_opt_set_arrays = [i.to_array() for i in java_opt_sets]
  opt_set_arrays = [i.to_array() for i in opt_sets]
  for java_opt_list in itertools.product(*java_opt_set_arrays):
    for opt_list in itertools.product(*opt_set_arrays):
      results_token = "results: "
      # TODO(andy): Add a timout on the subprocess.
      classpath = "%s/target/perf-tests-assembly.jar" % proj_dir
      cmd = "%s %s -cp %s %s %s %s" % (config.SCALA_CMD, " ".join(java_opt_list), classpath,
          test_cmd, config.SPARK_CLUSTER_URL, " ".join(opt_list))
      print("\nrunning command: %s\n" % cmd)
      output = Popen(cmd, stdout=PIPE, shell=True, env=new_env).stdout.read()
      if results_token not in output:
        print("Test did not produce expected results. Output was:")
        print(output)
        sys.exit(1)
      result_line = filter(lambda x: results_token in x, output.split("\n"))[0]
      result_list = result_line.replace(results_token, "").split(",")
      # TODO(andy): For even cardinality lists, return average of middle two elts.
      result_list = sorted([float(x) for x in result_list])
      result_med = result_list[len(result_list)/2]
      result_std = sqrt(variance(result_list))
      result_first = result_list[0]
      result_last = result_list[len(result_list) - 1]
      result_string = "%s, %s, %s, %.3f, %s, %s" % (short_name, " ".join(opt_list), result_med,
          result_std, result_first, result_last)
      print(result_string)
      out_file.write(result_string + "\n")
      sys.stdout.flush()
      out_file.flush()

print("Finished running all tests. See CSV output in %s" % config.OUTPUT_FILENAME)
print("We left your test cluster running. You should be able to kill it by running ./spark/bin/stop-all.sh")
