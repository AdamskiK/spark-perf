#!/usr/bin/env python

import itertools
import os.path
import sys
from subprocess import Popen, PIPE

if len(sys.argv) != 3:
  sys.exit("Usage: ./run [Git Commit Hash] [Output Filename]")

commit_to_test = sys.argv[1]
output_filename = sys.argv[2]

proj_dir = os.path.dirname(os.path.abspath(__file__)) + "/.." 
sbt_cmd = "sbt/sbt"

# Run shell command and ignore output
def run_cmd(cmd):
  Popen(cmd, stdout=sys.stderr, shell=True).wait()

# Check if a config file exists
if not os.path.isfile(proj_dir+"/config/config.py"):
  sys.exit("Please create a config file called %s/config/config.py" % proj_dir)

# import the configuration settings
sys.path.append(proj_dir + "/config")
import config

print("Downloading Spark...\n")
# Clone or fetch updates for the spark repository.
if not os.path.isdir("spark"):
  run_cmd("git clone %s", config.SPARK_GIT_REPO)
else:
  run_cmd("cd spark; git fetch origin")

# Build Spark
print("Cleaning Spark and building branch %s. This may take a while...\n" % commit_to_test)
run_cmd("cd spark; git clean -f -d -x; git reset --hard %s; %s clean package" % (commit_to_test, sbt_cmd))

# Copy Spark configuration files to new directory
print("Copying all files from %s to %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))
if not os.path.exists("%s/spark-env.sh" % config.SPARK_CONF_DIR):
  sys.exit("Could not find Spark config directory: %s" % config.SPARK_CONF_DIR)
run_cmd("cp %s/* %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))

# Building the perf-tests code against the downloaded version of Spark.
print("Building spark-perf...\n")
run_cmd("cd %s; %s package" % (proj_dir, sbt_cmd))

# Sync the whole directory to the slaves. 
# Here we are assuming we are using our Amazon EC2 AMI, fixing this is a TODO.
if not os.path.exists("%s/spark-ec2/copy-dir" % proj_dir):
  sys.exit("%s/spark-ec2/copy-dir not found, giving up." % proj_dir)
print("Syncing the test directory to the slaves.")
run_cmd("%s/spark-ec2/copy-dir %s" % (proj_dir, proj_dir))


# Set Spark Java Options (from config.py)
new_env = os.environ.copy()

new_env["SPARK_JAVA_OPTS"] = ""
for k, v in config.JAVA_OPTS.iteritems():
  new_env["SPARK_JAVA_OPTS"] += "-D%s=%s " % (k, v)
new_env["SPARK_HOME"] = "%s/spark" % proj_dir

# Clear the Spark local directories.
def clear_dir (dir_name):
  print("Clearing dir %s.\n" % dir_name)
  run_cmd("rm -r %s/*" % dir_name)
  run_cmd("%s/spark-ec2/copy-dir %s" % (proj_dir, dir_name))

spark_env = open("%s/spark-env.sh" % config.SPARK_CONF_DIR, 'r').read() 
re_result = re.search(r'spark.local.dir=([^"]*)"', spark_env)
if re_result:
  spark_local_dirs = re_result.group(1).split(",")
  map(clear_dir, spark_local_dirs)

# Setup the Spark master variable based on the spark-env.sh file
# from the specified Spark conf directory.
cluster_url = open("/root/spark-ec2/cluster-url", 'r').readline().strip()
print("cluster_url set to %s from /root/spark-ec2/cluster-url" % cluster_url)

# Try shutting down the cluster started by the Spark EC2 scripts,
# just in case it is still running.
if os.path.exists("/root/spark/bin/stop-all.sh"):
  run_cmd("/root/spark/bin/stop-all.sh" % proj_dir)

# Restart our test Spark cluster.
print("Restarting standalone scheduler...")
run_cmd("%s/spark/bin/stop-all.sh" % proj_dir)
run_cmd("%s/spark/bin/start-all.sh" % proj_dir)

# Some utility functions to calculate useful stats on test output.
def average(in_list):
  return sum(in_list) / len(in_list)

def variance(in_list):
  variance = 0
  for x in in_list:
    variance = variance + (average(in_list) - x) ** 2
  return variance / len(in_list)

# Run all tests specified in the Config file.
out_file = open(output_filename, 'w')
for short_name, test_cmd, option_sets in config.TESTS:
  print("\n---------------------------------------------")
  print("Running all tests for %s.\n" % test_cmd)
  # Run a test for all combinations of the OptionSets given, then capture
  # and print the output.
  option_set_arrays = [i.toArray() for i in option_sets]
  for opt_list in itertools.product(*option_set_arrays):
    results_token = "results: "
    # TODO(andy): Add a timout on the subprocess.
    cmd = '%s "run %s %s %s"' % (sbt_cmd,
                                 test_cmd,
                                 cluster_url,
                                 " ".join(opt_list))
    print("\nrunning command: %s\n" % cmd)
    output = Popen(cmd, stdout=PIPE, shell=True, env=new_env).stdout.read()
    if results_token not in output:
      print("Test did not produce expected results. Output was:")
      print(output)
      sys.exit(1)
    result_line = filter(lambda x: results_token in x, output.split("\n"))[0]
    result_list = result_line.replace(results_token, "").split(",")
    # TODO(andy): For even cardinality lists, return average of middle two elts.
    result_list = sorted([float(x) for x in result_list])
    result_med = result_list[len(result_list)/2]
    result_var = variance(result_list)
    result_first = result_list[0]
    result_last = result_list[len(result_list) - 1]
    result_string = "%s, %s, %s, %s, %s, %s" % (short_name, " ".join(opt_list), result_med, result_var, result_first, result_last)
    print(result_string)
    out_file.write(result_string + "\n")
    sys.stdout.flush()
    out_file.flush()

print("Finished running all tests. See CSV output in %s" % output_filename)
